\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{xspace}
\usepackage{tikz}

\begin{document}

\title{Notes on the Backward View of SARSA}

\author{}

\maketitle

\section{Intro}
In the forward view of RL we imagine we're in some state $S_t$, we take some action $a_t$, receive some reward $R_t$,
and land in some new state $S_{t+1}$.

$\text{TD}(0)$ says that we should update $V(S_t)$ to be equal to $V(S_t) + \alpha (G^1_t - V(S_t))$ where the \emph{TD target}
at step $t$ is $G^1_t = R_t + \gamma V(S_{t+1})$.
We also will notate $\delta^1_t = G^1_t - V(S_t)$, such that the update becomes $V(S_t) \rightarrow V(S_t) + \alpha\delta^1_t$.

We can, however, also use $n$-step rewards with $n>1$. For example, with $n=2$ we would have
$G^2_t = R_t + \gamma R_{t+1} + \gamma^2 V(S_{t+2})$.

In general the $n$-step return would be $G^n_t = \gamma^n V(S_{t+n}) + \sum_{k=0}^{n - 1} \gamma^k R_{t+k}$.
So which $n$ is best? We could use $1$, as in $\text{TD}(0)$, or we could use $\infty$ and do full Monte Carlo backups.
Instead, we can generalize to use $\text{TD}(\lambda)$ which weights all the targets using an expontentially decreasing
weighting with ratio $\lambda$: $G_t = (1-\lambda) \sum_{i=1}^{\infty} \lambda^{i-1}G^i_t$.
Note that the extra factor of $1 - \lambda$ serves to normalize the weights such that $(1 - \lambda) \sum_{i=1} \lambda^{i-1} = 1$.

Also note that when an episode ends, the last $G_T$ should receive all remaining weight.
If the final reward is $R_{t+k}$ then the last weight should be 1 - $\sum_{i=0}^{k - 1} (1 - \lambda) \lambda^i = \lambda^k$.

This is the forward view of $\text{TD}(\lambda)$.
It works fine, but it has the problem that we can't do online updates; we must wait until the end of an episode to be able to compute $G_t$.
Instead we would like to work out the backward view of the same alrgorithm.
Instead of updating $V(S_t)$ w.r.t. all the future rewards $R_{t+k}$,
we instead update all previous states whenever we receive a reward.

To do this we will first begin by considering the update we wish to apply: $V(S_t) \rightarrow V(S_t) + \alpha\big(G_t - V(S_t)\big)$.
In particular consider the \emph{TD error} $G_t - V(S_t)$.
For notational convience we will define $G^0_t = V(S_t)$.

\begin{align*}
G_t - V(S_t) &= (1-\lambda)\sum_{i=1}^\infty \big[\lambda^{i-1} G^i_t \big] - G^0_t \\
&= -G^0_t + (1 - \lambda)\lambda^0 G^1_t + (1 - \lambda)\lambda^1 G^2_t + (1 - \lambda)\lambda^2 G^3_t + \dots \\
&= -\lambda^0 G^0_t + (\lambda^0 - \lambda^1) G^1_t + (\lambda^1 - \lambda^2) G^2_t + (\lambda^2 - \lambda^3) G^3_t + \dots \\
&= \lambda^0(G^1_t - G^0_t) + \lambda^1(G^2_t - G^1_t) + \lambda^2(G^3_t - G^2_t) + \lambda^3(G^4_t - G^3_t) + \dots \\
&= \sum_{i=1}^\infty \lambda^{i-1}(G^i_t - G^{i-1}_t) \\
&= \sum_{i=1}^\infty \lambda^{i-1}\bigg[\big(\gamma^i V(S_{t+i}) + \sum_{k=0}^{i - 1} \gamma^k R_{t+k}\big) - \big(\gamma^{i-1} V(S_{t+i-1}) + \sum_{k=0}^{i - 2} \gamma^k R_{t+k}\big) \bigg] \\
&= \sum_{i=1}^\infty \lambda^{i-1}\bigg[\big(\gamma^i V(S_{t+i}) - \gamma^{i-1} V(S_{t+i-1}) \big) + \big(\sum_{k=0}^{i - 1} \gamma^k R_{t+k} - \sum_{k=0}^{i - 2} \gamma^k R_{t+k}\big) \bigg] \\
&= \sum_{i=1}^\infty \lambda^{i-1}\bigg[\gamma^{i-1}\big(\gamma V(S_{t+i}) - V(S_{t+i-1}) \big) + \big(\gamma^{i-1} R_{t + i - 1}\big) \bigg] \\
&= \sum_{i=1}^\infty \lambda^{i-1}\gamma^{i-1}\bigg[\gamma V(S_{t+i}) - V(S_{t+i-1}) + R_{t + i - 1} \bigg] \\
&= \sum_{i=1}^\infty (\lambda\gamma)^{i-1}\bigg[\big(\gamma V(S_{t+i}) + R_{t + i - 1}\big) - V(S_{t+i-1}) \bigg] \\
&= \sum_{i=1}^\infty (\lambda\gamma)^{i-1}\delta^1_{t + i - 1} \\
\end{align*}

This means that at each time step we can simply update $V(s)$ for each previous states $s$ by $(\lambda\gamma)^n\delta^1_t$,
where $n$ is the number of intervening time steps.

\section{Old Stuff}

Let's expand the definition of the overall $G_t$:

\begin{align*}
G_t &= (1-\lambda) \sum_{i=1}^{\infty} \lambda^{i-1}G^i_t \\
&= (1-\lambda) \sum_{i=1}^{\infty} \lambda^{i-1} \big[ \gamma^i V(S_{t+i}) + \sum_{k=0}^{i - 1} \gamma^k R_{t+k} \big] \\
&= (1-\lambda) \sum_{i=1}^{\infty} \big[ \lambda^{i-1} \gamma^i V(S_{t+i}) + \lambda^{i-1}\sum_{k=0}^{i - 1} \gamma^k R_{t+k} \big] \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} \big[ (\lambda \gamma)^i V(S_{t+i}) + \lambda^{i}\sum_{k=0}^{i - 1} \gamma^k R_{t+k} \big] \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{i=1}^{\infty} (1 - \lambda) \lambda^{i - 1}\sum_{k=0}^{i - 1} \gamma^k R_{t+k} \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{k=0}^\infty \big[ \sum_{i=k}^\infty (1 - \lambda)\lambda^i \big]\gamma^k R_{t+k} \\
\end{align*}

Note that this means that each $\gamma^k R_{t+k}$ term will be added in for all $i \geq k$.
Alternatively we might look at as it will be skipped only when $i < k$.
This means that $\gamma^k R_{t+k}$ will be added in with a total weight of $1 - \sum_{i=0}^{k - 1} (1 - \lambda) \lambda^{i} = \lambda^k$

\begin{align*}
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{k=0}^\infty \lambda^k \gamma^k R_{t+k} \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{k=0}^\infty (\lambda \gamma)^k R_{t+k} \\
&= R_t + \sum_{k=1}^{\infty} (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg) \\
\end{align*}

So the total update done to $V(S_t)$ is:

\begin{align*}
\Delta V(S_t) &= \alpha\big(G_t - V(S_t)\big) \\
&= \alpha\Big(R_t - V(S_t) + \sum_{k=1}^{\infty} (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg) \Big) \\
&= \alpha\big(R_t - V(S_t) \big) + \alpha \sum_{k=1}^{\infty} (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg)  \\
\end{align*}

So upon departing state $S_t$ and receiving reward $R_t$ we can immediately update $V(S_t)$ by $\alpha\big(R_t - V(S_t)\big)$.
Then from then on, whenever we leave state $S_{t+k}$ and receive reward $R_{t +k}$ we again update $V(S_t)$ by
$\alpha (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg)$.

When we end an episode at state $S_{t+k}$ then $V(S_{t+k})$ is trivially 0.
We also upweight the update by $\frac{1}{1 - \lambda}$, giving all remaining weight to this update.
Thus the update becomes $\frac{\alpha}{1 - \lambda} (\lambda \gamma)^k R_{t+k}$

\section{Another try}
Let's define $G^0_t = V(S_t)$.
At each timestep $t + n$ let's do a full update towards $G^n_t$.
Then when we progress to the next time step we'll undo a fraction of that update as we perform the update towards $G^{n+1}_t$.

As of time step $t + n$ the total weight used so far is

\begin{align*}
\sum_{k=1}^n (1 - \lambda) \lambda^{k - 1} &= (1 - \lambda) \big[ \sum_{k=1}^\infty \lambda^{k - 1} - \sum_{k=n+1}^\infty \lambda^{k - 1} \big] \\
&= (1 - \lambda) \big[ \frac{1}{1 - \lambda} - \frac{\lambda^n}{1 - \lambda} \big] \\
&= 1 - \lambda^n
\end{align*}

which means the remaining weight is $\lambda^n$.

Consider the quantity

\begin{align*}
\sum_{n=1}^\infty \lambda^{n-1} (G^n_t - G^{n-1}_t) &= (G^1_t - G^0_t) + \lambda (G^2_t - G^1_t) + \lambda^2 (G^3_t - G^2_t) + \dots \\
&= -G^0_t + (1 - \lambda)G^1_t + (\lambda - \lambda^2)G^2_t + (\lambda^2 - \lambda^3)G^3_t + \dots \\
&= -G^0_t + (1 - \lambda)G^1_t + (1 - \lambda)\lambda G^2_t + (1 - \lambda)\lambda^2 G^3_t + \dots \\
&= -G^0_t + (1 - \lambda)\sum_{n=1}^\infty \lambda^{n - 1} G^n_t \\
&= -G^0_t + G_t = G_t - V(S_t)
\end{align*}

and of course the update we want to do is $V(S_t) \leftarrow V(S_t) + \alpha\big(G_t - V(S_t)\big)$.
We can now rewrite this as $V(S_t) \leftarrow V(S_t) + \alpha \sum_{n=1}^\infty \lambda^{n-1} (G^n_t - G^{n-1}_t)$.

Now let's expand the quantity

\begin{align*}
G^n_t - G^{n-1}_t &= \big[\gamma^n V(S_{t+n}) + \sum_{k=0}^{n - 1} \gamma^k R_{t+k} \big] - \big[\gamma^{n-1} V(S_{t+n-1}) + \sum_{k=0}^{n - 2} \gamma^k R_{t+k} \big] \\
&= \gamma^{n-1} (\gamma V(S_{t + n}) - V(S_{t + n - 1})) + \gamma^{n - 1} R_{t + n - 1}  \\
&= \gamma^{n-1} (R_{t + n - 1} + \gamma V(S_{t + n}) - V(S_{t + n - 1})) \\
\end{align*}

So now the update is $V(S_t) \leftarrow V(S_t) + \alpha \sum_{n=1}^\infty (\lambda\gamma)^{n-1} \big(R_{t + n - 1} + \gamma V(S_{t + n}) - V(S_{t + n - 1})\big)$.

This now affords us the ability to do the updates online.
At each timestep we loop over previous states and update their values by $\alpha(\lambda\gamma)^k \big(R_t + \gamma V(S_{t + 1}) - V(S_t) \big)$ where $k$ is the number
of intervening steps.

\end{document}
