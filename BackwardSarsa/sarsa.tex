\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{xspace}
\usepackage{tikz}

\begin{document}

\title{Notes on the Backward View of SARSA}

\author{}

\maketitle

\section{Intro}
In the forward view of RL we imagine we're in some state $S_t$, we take some action $a_t$, receive some reward $R_t$,
and land in some new state $S_{t+1}$.

$\text{TD}(0)$ says that we should update $V(S_t)$ to be equal to $V(S_t) + \alpha (G_t - V(S_t))$ where the \emph{TD target}
at step $t$ is $G^1_t = R_t + \gamma V(S_{t+1})$.

We can, however, also use $n$-step rewards with $n>1$. For example, with $n=2$ we would have
$G^2_t = R_t + \gamma R_{t+1} + \gamma^2 V(S_{t+2})$.

In general the $n$-step return would be $G^n_t = \gamma^n V(S_{t+n}) + \sum_{k=0}^{n - 1} \gamma^k R_{t+k}$.
So which $n$ is best? We could use $1$, as in $\text{TD}(0)$, or we could use $\infty$ and do full Monte Carlo backups.
Instead, we can generalize to use $\text{TD}(\lambda)$ which weights all the targets using an expontentially decreasing
weighting with ratio $\lambda$: $G_t = (1-\lambda) \sum_{i=1}^{\infty} \lambda^{i-1}G^i_t$.
Note that the extra factor of $1 - \lambda$ serves to normalize the weights such that $(1 - \lambda) \sum_{i=1} \lambda^{i-1} = 1$.

Also note that when an episode ends, the last $G_T$ should receive all remaining weight.
I need to go back and figure out how this works mathematically.
If the final reward is $R_{t+k}$ then the last weight should be 1 - $\sum_{i=0}^{k - 1} (1 - \lambda) \lambda^i = \lambda^k$.

This is the forward view of $\text{TD}(\lambda)$.
It works fine, but it has the problem that we can't do online updates; we must wait until the end of an episode to be able to compute $G_t$.
Instead we would like to work out the backward view of the same alrgorithm.
Instead of updating $V(S_t)$ w.r.t. all the future rewards $R_{t+k}$,
we instead update all previous states whenever we receive a reward.
We simply need to work out what the correct update is to each previous state $S_{t -k}$ w.r.t. a current reward $R_t$.
This should just be simple algebra, so I will work it out here.

Let's expand the definition of the overall $G_t$:

\begin{align*}
G_t &= (1-\lambda) \sum_{i=1}^{\infty} \lambda^{i-1}G^i_t \\
&= (1-\lambda) \sum_{i=1}^{\infty} \lambda^{i-1} \big[ \gamma^i V(S_{t+i}) + \sum_{k=0}^{i - 1} \gamma^k R_{t+k} \big] \\
&= (1-\lambda) \sum_{i=1}^{\infty} \big[ \lambda^{i-1} \gamma^i V(S_{t+i}) + \lambda^{i-1}\sum_{k=0}^{i - 1} \gamma^k R_{t+k} \big] \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} \big[ (\lambda \gamma)^i V(S_{t+i}) + \lambda^{i}\sum_{k=0}^{i - 1} \gamma^k R_{t+k} \big] \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{i=1}^{\infty} (1 - \lambda) \lambda^{i - 1}\sum_{k=0}^{i - 1} \gamma^k R_{t+k} \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{k=0}^\infty \big[ \sum_{i=k}^\infty (1 - \lambda)\lambda^i \big]\gamma^k R_{t+k} \\
\end{align*}

Note that this means that each $\gamma^k R_{t+k}$ term will be added in for all $i \geq k$.
Alternatively we might look at as it will be skipped only when $i < k$.
This means that $\gamma^k R_{t+k}$ will be added in with a total weight of $1 - \sum_{i=0}^{k - 1} (1 - \lambda) \lambda^{i} = \lambda^k$

\begin{align*}
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{k=0}^\infty \lambda^k \gamma^k R_{t+k} \\
&= \frac{(1-\lambda)}{\lambda} \sum_{i=1}^{\infty} (\lambda \gamma)^i V(S_{t+i}) + \sum_{k=0}^\infty (\lambda \gamma)^k R_{t+k} \\
&= R_t + \sum_{k=1}^{\infty} (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg) \\
\end{align*}

So the total update done to $V(S_t)$ is:

\begin{align*}
\Delta V(S_t) &= \alpha\big(G_t - V(S_t)\big) \\
&= \alpha\Big(R_t - V(S_t) + \sum_{k=1}^{\infty} (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg) \Big) \\
&= \alpha\big(R_t - V(S_t) \big) + \alpha \sum_{k=1}^{\infty} (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg)  \\
\end{align*}

So upon departing state $S_t$ and receiving reward $R_t$ we can immediately update $V(S_t)$ by $\alpha\big(R_t - V(S_t)\big)$.
Then from then on, whenever we leave state $S_{t+k}$ and receive reward $R_{t +k}$ we again update $V(S_t)$ by
$\alpha (\lambda \gamma)^k \bigg(\big(\frac{1 - \lambda}{\lambda}\big)V(S_{t+k}) + R_{t+k} \bigg)$.

When we end an episode at state $S_{t+k}$ then $V(S_{t+k})$ is trivially 0.
We also upweight the update by $\frac{1}{1 - \lambda}$, giving all remaining weight to this update.
Thus the update becomes $\frac{\alpha}{1 - \lambda} (\lambda \gamma)^k R_{t+k}$

\section{Another try}
Let's define $G^0_t = V(S_t)$.
At each timestep $t + n$ let's do a full update towards $G^n_t$.
Then when we progress to the next time step we'll undo a fraction of that update as we perform the update towards $G^{n+1}_t$.

As of time step $t + n$ the total weight used so far is

\begin{align*}
\sum_{k=1}^n (1 - \lambda) \lambda^{k - 1} &= (1 - \lambda) \big[ \sum_{k=1}^\infty \lambda^{k - 1} - \sum_{k=n+1}^\infty \lambda^{k - 1} \big] \\
&= (1 - \lambda) \big[ \frac{1}{1 - \lambda} - \frac{\lambda^n}{1 - \lambda} \big] \\
&= 1 - \lambda^n
\end{align*}

which means the remaining weight is $\lambda^n$.

Consider the quantity

\begin{align*}
\sum_{n=1}^\infty \lambda^{n-1} (G^n_t - G^{n-1}_t) &= (G^1_t - G^0_t) + \lambda (G^2_t - G^1_t) + \lambda^2 (G^3_t - G^2_t) + \dots \\
&= -G^0_t + (1 - \lambda)G^1_t + (\lambda - \lambda^2)G^2_t + (\lambda^2 - \lambda^3)G^3_t + \dots \\
&= -G^0_t + (1 - \lambda)G^1_t + (1 - \lambda)\lambda G^2_t + (1 - \lambda)\lambda^2 G^3_t + \dots \\
&= -G^0_t + (1 - \lambda)\sum_{n=1}^\infty \lambda^{n - 1} G^n_t \\
&= -G^0_t + G_t = G_t - V(S_t)
\end{align*}

and of course the update we want to do is $V(S_t) \leftarrow V(S_t) + \alpha\big(G_t - V(S_t)\big)$.
We can now rewrite this as $V(S_t) \leftarrow V(S_t) + \alpha \sum_{n=1}^\infty \lambda^{n-1} (G^n_t - G^{n-1}_t)$.

\end{document}
